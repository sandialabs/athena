{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faf1f5f2-8271-475e-b341-e73ff6bcacde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sonos_accelerator import helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa1b090d-5bdc-4e76-b96c-1e28df18afca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sonos_accelerator.helpers.dataflow_plots import plotDataflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1a5057a-54b7-4155-bffe-bd3962e42eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sonos_accelerator.neural_networks.specifyModel_VGG import specifyModel_VGG\n",
    "from sonos_accelerator.neural_networks.specifyModel_ResNet50 import specifyModel_ResNet50\n",
    "from sonos_accelerator.neural_networks.specifyModel_ResNet50v15 import specifyModel_ResNet50v15\n",
    "from sonos_accelerator.neural_networks.specifyModel_ResNet34 import specifyModel_ResNet34\n",
    "from sonos_accelerator.neural_networks.specifyModel_custom import specifyModel_custom\n",
    "from sonos_accelerator.buildAccelerator import allocateHardware, allocateTiles, calculateTileEnergyArea\n",
    "from sonos_accelerator.dataflow import simulateDataflow\n",
    "from sonos_accelerator.helpers.dataflow_plots import plotDataflow\n",
    "from sonos_accelerator.helpers.eval_energy import eval_energy\n",
    "\n",
    "def cli(model=\"VGG16\"):\n",
    "\t########################################\n",
    "\t##\n",
    "\t##\tChoose neural network model\n",
    "\t##\n",
    "\t########################################\n",
    "\n",
    "\t# Definitions:\n",
    "\t#\tarrayDims : array dimension to be used in accelerator (not including negative array)\n",
    "\t#\tNcycles_target : target # compute cycles to finish a given conv layer, \n",
    "\t#\t\tused to set weight replication factor\n",
    "\n",
    "\t# model = \"ResNet34\"\n",
    "\t# model = \"ResNet50\"\n",
    "\t# model = \"ResNet50v15\"\n",
    "\t#model = \"VGG16\"\n",
    "\t# model = \"custom\"\n",
    "\n",
    "\t# Load previously saved dataflow results\n",
    "\tloadFromSaved = False\n",
    "\tNimages = 1\n",
    "\n",
    "\tarchParams = {}\n",
    "\n",
    "\t# Sliding window allocation order\n",
    "\t# Affects delay, latency, and buffering\n",
    "\t# 1: Nbx parallel to Ntx\n",
    "\t# 2: Nbx orthogonal to Ntx\n",
    "\n",
    "\tif model == \"VGG16\":\n",
    "\t\tNN_layers = specifyModel_VGG()\n",
    "\t\tarrayDims = np.array([1152,256])\n",
    "\t\tNcycles_delay = 39\n",
    "\n",
    "\t\tNcycles_target = 112\n",
    "\n",
    "\t\tif Ncycles_target == 112:\n",
    "\t\t\tarchParams['SWorder'] = 2\n",
    "\t\t\tarchParams['memorySizeKb'] = 64\n",
    "\t\t\tarchParams['receiveBufferSizeKb'] = 8\n",
    "\t\t\tarchParams['tileOutBufferSizeKb'] = 4\n",
    "\t\t\tarchParams['maxMPoutputs'] = 1000\n",
    "\t\t\tarchParams['Nbc_pool_factor'] = 50000\n",
    "\t\t\tarchParams['A_router'] = 107526e-12\n",
    "\t\t\tarchParams['P_router'] = 11.8e-3*2\n",
    "\n",
    "\telif model == \"ResNet50\":\n",
    "\t\tNN_layers = specifyModel_ResNet50()\n",
    "\t\tarrayDims = np.array([1152,256])\n",
    "\t\tNcycles_target = 28\n",
    "\t\tNcycles_delay = 0\n",
    "\telif model == \"ResNet50v15\":\n",
    "\t\tNN_layers = specifyModel_ResNet50v15()\n",
    "\t\tarrayDims = np.array([1152,256])\n",
    "\t\tNcycles_target = 112\n",
    "\t\tarchParams['SWorder'] = 1\n",
    "\n",
    "\t\tif Ncycles_target == 224:\n",
    "\t\t\tarchParams['memorySizeKb'] = 64\n",
    "\t\t\tarchParams['receiveBufferSizeKb'] = 6\n",
    "\t\t\tarchParams['maxMPoutputs'] = int(np.ceil(2*384))\n",
    "\t\t\tarchParams['mergeTileFactor'] = np.array([4,4,4,4,2,2,2,2,2,2,2,2,2,2,2,2])\n",
    "\t\t\tarchParams['Nbc_pool_factor'] = 200000\n",
    "\t\t\tarchParams['A_router'] = 64479.1e-12\n",
    "\t\t\tarchParams['P_router'] = 11.8e-3\n",
    "\t\telif Ncycles_target == 112:\n",
    "\t\t\tarchParams['memorySizeKb'] = 64\n",
    "\t\t\tarchParams['receiveBufferSizeKb'] = 4\n",
    "\t\t\tarchParams['maxMPoutputs'] = 2*384\n",
    "\t\t\tarchParams['mergeTileFactor'] = np.array([2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2])\n",
    "\t\t\tarchParams['Nbc_pool_factor'] = 200000\n",
    "\t\t\tarchParams['A_router'] = 64479.1e-12\n",
    "\t\t\tarchParams['P_router'] = 11.8e-3\n",
    "\t\telif Ncycles_target == 56:\n",
    "\t\t\tarchParams['memorySizeKb'] = 64\n",
    "\t\t\tarchParams['A_router'] = 107526e-12\n",
    "\t\t\tarchParams['P_router'] = 11.8e-3*2\n",
    "\t\t\tarchParams['receiveBufferSizeKb'] = 6\n",
    "\t\t\tarchParams['maxMPoutputs'] = int(np.ceil(2*384))\n",
    "\t\t\tarchParams['mergeTileFactor'] = np.array([1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1])\n",
    "\t\t\tarchParams['Nbc_pool_factor'] = 100000\n",
    "\t\telif Ncycles_target == 52:\n",
    "\t\t\tarchParams['memorySizeKb'] = 64\n",
    "\t\t\tarchParams['A_router'] = 107526e-12\n",
    "\t\t\tarchParams['P_router'] = 11.8e-3*2\n",
    "\t\t\tarchParams['receiveBufferSizeKb'] = 4\n",
    "\t\t\tarchParams['maxMPoutputs'] = int(np.ceil(2*384))\n",
    "\t\t\tarchParams['mergeTileFactor'] = np.array([1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1])\n",
    "\t\t\tarchParams['Nbc_pool_factor'] = 100000\n",
    "\t\telif Ncycles_target == 28:\n",
    "\t\t\tarchParams['memorySizeKb'] = 64\n",
    "\t\t\tarchParams['receiveBufferSizeKb'] = 6.125\n",
    "\t\t\tarchParams['maxMPoutputs'] = int(np.ceil(1.5*384))\n",
    "\t\t\tarchParams['mergeTileFactor'] = np.array([1,1,1,1,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,1,1,1])\n",
    "\t\t\tarchParams['Nbc_pool_factor'] = 100000\n",
    "\t\t\tarchParams['A_router'] = 107526e-12\n",
    "\t\t\tarchParams['P_router'] = 11.8e-3*2\n",
    "\n",
    "\t\tNcycles_delay = 105 # 39 if no shift reg, 34 with shift reg\n",
    "\telif model == \"ResNet34\":\n",
    "\t\tNN_layers = specifyModel_ResNet34()\n",
    "\t\tarrayDims = np.array([1152,256])\n",
    "\t\tNcycles_delay = 39\n",
    "\n",
    "\t\tNcycles_target = 110\n",
    "\n",
    "\t\tarchParams['SWorder'] = 1\n",
    "\t\tarchParams['memorySizeKb'] = 64\n",
    "\t\tarchParams['receiveBufferSizeKb'] = 2\n",
    "\t\tarchParams['maxMPoutputs'] = 2*384\n",
    "\t\tarchParams['mergeTileFactor'] = np.array([2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2])\n",
    "\t\tarchParams['Nbc_pool_factor'] = 200000\n",
    "\t\tarchParams['A_router'] = 64479.1e-12\n",
    "\t\tarchParams['P_router'] = 11.8e-3\n",
    "\n",
    "\telif model == \"custom\":\n",
    "\t\tNN_layers = specifyModel_custom()\n",
    "\t\tarrayDims = np.array([1152,256])\n",
    "\t\tNcycles_target = 112\n",
    "\t\t# For this network,\n",
    "\t\t# SWorder = 2 is faster (126 cycles) but less efficient (59.1 TOPS/W)\n",
    "\t\t# SWorder = 1 is slower (172 cycles) but more efficient ()\n",
    "\t\tarchParams['SWorder'] = 1\n",
    "\t\tarchParams['memorySizeKb'] = 64\n",
    "\t\tarchParams['receiveBufferSizeKb'] = 6\n",
    "\t\tarchParams['tileOutBufferSizeKb'] = 1\n",
    "\t\tarchParams['maxMPoutputs'] = 1000\n",
    "\t\tarchParams['Nbc_pool_factor'] = 200000\n",
    "\t\tarchParams['A_router'] = 107526e-12\n",
    "\t\tarchParams['P_router'] = 11.8e-3*2\n",
    "\t\tNcycles_delay = 0\n",
    "\telse:\n",
    "\t\traise ValueError(\"This neural network is not yet supported\")\n",
    "\n",
    "\t########################################\n",
    "\t##\n",
    "\t##\tSet architecture parameters\n",
    "\t##\n",
    "\t########################################\n",
    "\n",
    "\t# Create architecture parameter object\n",
    "\t# Parameters\n",
    "\t#\theterogeneousTiling\t: if True (deprecated), four different array sizes are used, set in allocateHardware\n",
    "\t#\tNoutputsTile\t\t: number of array output values per tile; 1024 recommended\n",
    "\t#\t\t\t\t\t\t\tmay be greater than the number of activations produced per tile, if partial sums are added in-tile\n",
    "\t#\tNbits \t\t\t\t: number of activation bits, same as ADC resolution\t\t\t\t\t\t\n",
    "\t#\tbuffersizeKb \t\t: capacity of tile cache in Kbytes\n",
    "\t#\tt_clk \t\t\t\t: clock period\n",
    "\t#\tt_compCycle \t\t: length of compute cycle, i.e. pipeline stage\n",
    "\t#\tI_read \t\t\t\t: maximum read current\n",
    "\t#\tVdd_digital \t\t: digital supply\n",
    "\t#\tVdd_analog \t\t\t: analog supply\n",
    "\t#\tVref \t\t\t\t: DAC reference voltage\n",
    "\tarchParams['model'] = model\n",
    "\tarchParams['ArrayDims'] = arrayDims\n",
    "\tarchParams['NoutputsTile'] = 1024\n",
    "\tarchParams['heterogeneousTiling'] = False\n",
    "\tarchParams['Ncycles_target'] = Ncycles_target\n",
    "\tarchParams['Nbits'] = 8\n",
    "\tarchParams['Nbanks'] = 32\n",
    "\tarchParams['Ncycles_machine'] = 295\n",
    "\tarchParams['digital_bias'] = True\n",
    "\tarchParams['t_clk'] = 1e-9\n",
    "\tarchParams['I_read'] = 3200e-9\n",
    "\tarchParams['Vdd_digital'] = 1.1\n",
    "\tarchParams['Vdd_analog'] = 2.5\n",
    "\tarchParams['Vref'] = 1.0 # ADC reference voltage\n",
    "\n",
    "\tarchParams['CoreOutKb'] = 1.0\n",
    "\n",
    "\t# Process\n",
    "\tarchParams['A_lowV'] = 0.15e-12\n",
    "\tarchParams['A_highV'] = 0.653e-12\n",
    "\n",
    "\t# Area inflation factors:\n",
    "\t# 1) area_fraction_wiring is % of area dedicated to wiring/layout overhead in certain blocks\n",
    "\t#\tApplied everywhere except SONOS array and SRAM\n",
    "\t# 2) area_fraction_control is % of tile area dedicated to control unit and instruction memory\n",
    "\tarchParams['area_fraction_wiring'] = 0.2500\n",
    "\tarchParams['area_fraction_control'] = 0.0500\n",
    "\n",
    "\t# Data widths\n",
    "\tarchParams['NcoreOut_cycle'] = 8 # Number of outputs to send out of core each cycle\n",
    "\tarchParams['Nwrite_inputReg'] = 8 # Number of values written to input buffer per cycle\n",
    "\tarchParams['Nread_outputReg'] = 16 # Number of values read from output buffer per cycle\n",
    "\tarchParams['Nread_tileOut'] = 16 # Number of values to send out of tile per cycle\n",
    "\tarchParams['Nports_receiveBuffer'] = 16 # Number of values to read into the tile per cycle\n",
    "\n",
    "\t# Other settings\n",
    "\tarchParams['useShiftRegisters'] = True\n",
    "\tarchParams['weightReorder'] = True\n",
    "\tarchParams['Nimages'] = Nimages\n",
    "\tarchParams['Ncycles_delay'] = Ncycles_delay\n",
    "\t\t\n",
    "\t# Dataflow output file name\n",
    "\tif Nimages == 1:\n",
    "\t\tarchName = model+\"_\"+str(archParams['ArrayDims'][0])+\"x\"+str(archParams['ArrayDims'][1])+\"_\"+str(Ncycles_target)+\"cycles.npz\"\n",
    "\telse:\n",
    "\t\tarchName = model+\"_\"+str(archParams['ArrayDims'][0])+\"x\"+str(archParams['ArrayDims'][1])+\"_\"+str(Ncycles_target)+\"cycles_\"+str(Nimages)+\"images.npz\"\n",
    "\n",
    "\n",
    "\t########################################\n",
    "\t##\n",
    "\t##\tGenerate accelerator\n",
    "\t##\n",
    "\t########################################\n",
    "\n",
    "\t# Set overall hardware parameters\n",
    "\tNN_layers, layerParams, archParams = allocateHardware(NN_layers,archParams)\n",
    "\n",
    "\t# Allocate tiles  to layers\n",
    "\tNN_layers, layerParams, tiles = allocateTiles(NN_layers,layerParams,archParams)\n",
    "\n",
    "\t# Calculate the MVM energy consumption and area of the tiles\n",
    "\tlayerParams, archParams, area_layers = calculateTileEnergyArea(layerParams,NN_layers,archParams)\n",
    "\n",
    "\t########################################`\n",
    "\t##\n",
    "\t##\tSimulate neural network dataflow\n",
    "\t##\n",
    "\t########################################\n",
    "\n",
    "\tif loadFromSaved:\n",
    "\t\t# Load previously saved outputs\n",
    "\t\toutputs = np.load(\"./dataflow_outputs/\"+archName,allow_pickle=True)\n",
    "\t\tactivity = outputs[\"activity\"]\n",
    "\t\tprocessing = outputs[\"processing\"]\n",
    "\t\tbuffersizes = outputs[\"buffersizes\"]\n",
    "\t\ttraffic = outputs[\"traffic\"]\n",
    "\t\tenergies = outputs[\"energies\"]\n",
    "\t\tspeed = outputs[\"speed\"]\n",
    "\n",
    "\telse:\n",
    "\t\t# Simulate data flow\n",
    "\t\tactivity, processing, buffersizes, traffic, energies, speed = simulateDataflow(NN_layers,layerParams,tiles,archParams)\n",
    "\t\tnp.savez(\"./dataflow_outputs/\"+archName,activity=activity,buffersizes=buffersizes,traffic=traffic,energies=energies,processing=processing,speed=speed)\n",
    "\n",
    "\t# Plot dataflow\n",
    "\teval_energy(NN_layers,layerParams,archParams,energies,speed[1],Nimages=Nimages,concise=False)\n",
    "\tplotDataflow(activity,processing,buffersizes,traffic,speed,layerParams,NN_layers,Nimages,saveFigs=True,concise=True)\n",
    "\treturn layerParams, archParams, area_layers\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14b3cb05-fe2c-4ab5-ab9c-0d016b3f4734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer name:\tarray:\ttile:\tall:\tNbc:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/e/dev/athena/athena_tool/src/athena/athena_ir/sonos_accelerator/helpers/mathematical.py:9: NumbaWarning: \u001b[1m\n",
      "Compilation is falling back to object mode WITH looplifting enabled because Function \"findBlock\" failed type inference due to: \u001b[1mCan't unify return type from the following types: Literal[bool](False), UniTuple(int64 x 2)\n",
      "\u001b[1mReturn of: IR name '$62return_value.6', type 'UniTuple(int64 x 2)', location: \u001b[1m\n",
      "File \"sonos_accelerator/helpers/mathematical.py\", line 16:\u001b[0m\n",
      "\u001b[1mdef findBlock(N):\n",
      "    <source elided>\n",
      "\t\tF1 -= 1\n",
      "\u001b[1m\treturn F1, intDiv(N,F1)\n",
      "\u001b[0m \u001b[1m^\u001b[0m\u001b[0m\u001b[0m\n",
      "\u001b[1mReturn of: IR name '$12return_value.1', type 'Literal[bool](False)', location: \u001b[1m\n",
      "File \"sonos_accelerator/helpers/mathematical.py\", line 12:\u001b[0m\n",
      "\u001b[1mdef findBlock(N):\n",
      "    <source elided>\n",
      "\tif N < 1:\n",
      "\u001b[1m\t\treturn False\n",
      "\u001b[0m  \u001b[1m^\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
      "  @jit\n",
      "/mnt/e/dev/athena/athena_tool/src/athena/athena_ir/sonos_accelerator/helpers/mathematical.py:9: NumbaWarning: \u001b[1m\n",
      "Compilation is falling back to object mode WITHOUT looplifting enabled because Function \"findBlock\" failed type inference due to: \u001b[1m\u001b[1mCannot determine Numba type of <class 'numba.core.dispatcher.LiftedLoop'>\u001b[0m\n",
      "\u001b[1m\n",
      "File \"sonos_accelerator/helpers/mathematical.py\", line 13:\u001b[0m\n",
      "\u001b[1mdef findBlock(N):\n",
      "    <source elided>\n",
      "\t\treturn False\n",
      "\u001b[1m\tF1 = int(np.sqrt(N))\n",
      "\u001b[0m \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\n",
      "  @jit\n",
      "/home/mplagge/miniconda3/envs/athena_dev/lib/python3.9/site-packages/numba/core/object_mode_passes.py:151: NumbaWarning: \u001b[1mFunction \"findBlock\" was compiled in object mode without forceobj=True, but has lifted loops.\n",
      "\u001b[1m\n",
      "File \"sonos_accelerator/helpers/mathematical.py\", line 11:\u001b[0m\n",
      "\u001b[1mdef findBlock(N):\n",
      "\u001b[1m\tif N < 1:\n",
      "\u001b[0m \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(errors.NumbaWarning(warn_msg,\n",
      "/home/mplagge/miniconda3/envs/athena_dev/lib/python3.9/site-packages/numba/core/object_mode_passes.py:161: NumbaDeprecationWarning: \u001b[1m\n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
      "\n",
      "For more information visit https://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\u001b[1m\n",
      "File \"sonos_accelerator/helpers/mathematical.py\", line 11:\u001b[0m\n",
      "\u001b[1mdef findBlock(N):\n",
      "\u001b[1m\tif N < 1:\n",
      "\u001b[0m \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(errors.NumbaDeprecationWarning(msg,\n",
      "/mnt/e/dev/athena/athena_tool/src/athena/athena_ir/sonos_accelerator/helpers/mathematical.py:9: NumbaWarning: \u001b[1m\n",
      "Compilation is falling back to object mode WITH looplifting enabled because Function \"findBlock\" failed type inference due to: \u001b[1mCan't unify return type from the following types: Literal[bool](False), UniTuple(int64 x 2)\n",
      "\u001b[1mReturn of: IR name '$62return_value.6', type 'UniTuple(int64 x 2)', location: \u001b[1m\n",
      "File \"sonos_accelerator/helpers/mathematical.py\", line 16:\u001b[0m\n",
      "\u001b[1mdef findBlock(N):\n",
      "    <source elided>\n",
      "\t\tF1 -= 1\n",
      "\u001b[1m\treturn F1, intDiv(N,F1)\n",
      "\u001b[0m \u001b[1m^\u001b[0m\u001b[0m\u001b[0m\n",
      "\u001b[1mReturn of: IR name '$12return_value.1', type 'Literal[bool](False)', location: \u001b[1m\n",
      "File \"sonos_accelerator/helpers/mathematical.py\", line 12:\u001b[0m\n",
      "\u001b[1mdef findBlock(N):\n",
      "    <source elided>\n",
      "\tif N < 1:\n",
      "\u001b[1m\t\treturn False\n",
      "\u001b[0m  \u001b[1m^\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
      "  @jit\n",
      "/mnt/e/dev/athena/athena_tool/src/athena/athena_ir/sonos_accelerator/helpers/mathematical.py:9: NumbaWarning: \u001b[1m\n",
      "Compilation is falling back to object mode WITHOUT looplifting enabled because Function \"findBlock\" failed type inference due to: \u001b[1m\u001b[1mCannot determine Numba type of <class 'numba.core.dispatcher.LiftedLoop'>\u001b[0m\n",
      "\u001b[1m\n",
      "File \"sonos_accelerator/helpers/mathematical.py\", line 13:\u001b[0m\n",
      "\u001b[1mdef findBlock(N):\n",
      "    <source elided>\n",
      "\t\treturn False\n",
      "\u001b[1m\tF1 = int(np.sqrt(N))\n",
      "\u001b[0m \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\n",
      "  @jit\n",
      "/home/mplagge/miniconda3/envs/athena_dev/lib/python3.9/site-packages/numba/core/object_mode_passes.py:151: NumbaWarning: \u001b[1mFunction \"findBlock\" was compiled in object mode without forceobj=True, but has lifted loops.\n",
      "\u001b[1m\n",
      "File \"sonos_accelerator/helpers/mathematical.py\", line 11:\u001b[0m\n",
      "\u001b[1mdef findBlock(N):\n",
      "\u001b[1m\tif N < 1:\n",
      "\u001b[0m \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(errors.NumbaWarning(warn_msg,\n",
      "/home/mplagge/miniconda3/envs/athena_dev/lib/python3.9/site-packages/numba/core/object_mode_passes.py:161: NumbaDeprecationWarning: \u001b[1m\n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
      "\n",
      "For more information visit https://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\u001b[1m\n",
      "File \"sonos_accelerator/helpers/mathematical.py\", line 11:\u001b[0m\n",
      "\u001b[1mdef findBlock(N):\n",
      "\u001b[1m\tif N < 1:\n",
      "\u001b[0m \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(errors.NumbaDeprecationWarning(msg,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1:\t\t4, 4\t28, 16\t1\n",
      "Conv2:\t2, 2\t4, 4\t28, 16\t1\n",
      "Conv3:\t\t2, 4\t14, 8\t1\n",
      "Conv4:\t\t2, 2\t14, 8\t1\n",
      "Conv5:\t\t2, 2\t14, 2\t1\n",
      "Conv6:\t\t1, 2\t7, 4\t1\n",
      "Conv7:\t\t1, 2\t7, 4\t1\n",
      "Conv8:\t\t1, 1\t7, 1\t1\n",
      "Conv9:\t\t1, 1\t7, 1\t2\n",
      "Conv10:\t\t1, 1\t7, 1\t2\n",
      "Conv11:\t\t1, 1\t2, 1\t2\n",
      "Conv12:\t\t1, 1\t2, 1\t2\n",
      "Conv13:\t\t1, 1\t2, 1\t2\n",
      "----\n",
      "Effective tile area in cmesh: 2.316 mm^2\n",
      "Tile area: 1.844 mm^2\n",
      "  Cores + ramp: 0.731 mm^2 (39.645%)\n",
      "  Core: 0.727 mm^2 (39.406%)\n",
      "     per core: 0.182mm^2\n",
      "  RAM: 0.181 mm^2 (9.813%)\n",
      "  Buffers: 0.671 mm^2 (36.372%)\n",
      "     Receive FIFOs: 0.213 mm^2\n",
      "     MVMin buffer: 0.143 mm^2\n",
      "     ALUin buffer: 0.158 mm^2\n",
      "     TileOut buffer: 0.157 mm^2\n",
      "  Ramp generator: 0.004 mm^2 (0.238%)\n",
      "  ALU: 0.169 mm^2 (9.171%)\n",
      "     Adder1: 0.0020 mm^2\n",
      "     Adder2: 0.0012 mm^2\n",
      "     Bias Adder: 0.0232 mm^2\n",
      "     ReLU: 0.0115 mm^2\n",
      "     Range converter: 0.0791 mm^2\n",
      "     MaxPool: 0.0033 mm^2\n",
      "     AvgPool: 0.0064 mm^2\n",
      "  Control unit: 0.0922 mm^2 (5.000%)\n",
      "Total area: 769.030 mm^2\n",
      "  0: Conv1 (28 tiles), MVM: 54 x 64, area: 64.858 mm^2 (8.434%), util: 4.688%\n",
      "  1: Conv2 (28 tiles), MVM: 256.0 x 64, area: 64.858 mm^2 (8.434%), util: 50.000%\n",
      "  3: Conv3 (14 tiles), MVM: 576 x 128, area: 32.429 mm^2 (4.217%), util: 50.000%\n",
      "  4: Conv4 (28 tiles), MVM: 1152 x 128, area: 64.858 mm^2 (8.434%), util: 50.000%\n",
      "  6: Conv5 (7 tiles), MVM: 1152 x 256, area: 16.214 mm^2 (2.108%), util: 100.000%\n",
      "  7: Conv6 (14 tiles), MVM: 2304 x 256, area: 32.429 mm^2 (4.217%), util: 100.000%\n",
      "  8: Conv7 (14 tiles), MVM: 2304 x 256, area: 32.429 mm^2 (4.217%), util: 100.000%\n",
      "  9: MP3 (16 tiles): 37.062 mm^2 (4.819%)\n",
      "  10: Conv8 (7 tiles), MVM: 2304 x 512, area: 16.214 mm^2 (2.108%), util: 100.000%\n",
      "  11: Conv9 (14 tiles), MVM: 4608 x 512, area: 32.429 mm^2 (4.217%), util: 100.000%\n",
      "  12: Conv10 (14 tiles), MVM: 4608 x 512, area: 32.429 mm^2 (4.217%), util: 100.000%\n",
      "  13: MP4 (8 tiles): 18.531 mm^2 (2.410%)\n",
      "  14: Conv11 (4 tiles), MVM: 4608 x 512, area: 9.265 mm^2 (1.205%), util: 100.000%\n",
      "  15: Conv12 (4 tiles), MVM: 4608 x 512, area: 9.265 mm^2 (1.205%), util: 100.000%\n",
      "  16: Conv13 (4 tiles), MVM: 4608 x 512, area: 9.265 mm^2 (1.205%), util: 100.000%\n",
      "  17: MP5 (8 tiles): 18.531 mm^2 (2.410%)\n",
      "  18: FC1 (88 tiles), MVM: 25088 x 4096, area: 203.839 mm^2 (26.506%), util: 98.990%\n",
      "  19: Add1 (4 tiles): 9.265 mm^2 (1.205%)\n",
      "  20: FC2 (16 tiles), MVM: 4096 x 4096, area: 37.062 mm^2 (4.819%), util: 88.889%\n",
      "  21: Add2 (4 tiles): 9.265 mm^2 (1.205%)\n",
      "  22: FC3 (4 tiles), MVM: 4096 x 1000, area: 9.265 mm^2 (1.205%), util: 86.806%\n",
      "  23: Add3 (1 tiles): 2.316 mm^2 (0.301%)\n",
      "Total # layers: 24\n",
      "Total # tiles: 332\n",
      "Total # tiles used: 329\n",
      "   Conv1 tiles: 0.000%\n",
      "   Conv3+ tiles: 54.217%\n",
      "   Merge tiles: 2.711%\n",
      "   Pool tiles: 9.639%\n",
      "   FC tiles: 32.530%\n",
      "Fraction of tiles analog: 86.747%\n",
      "Overall array utilization: 67.204%\n",
      "   Conv1 tiles: nan%\n",
      "   Conv3+ tiles: 65.729%\n",
      "   FC tiles: 97.042%\n",
      "----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/e/dev/athena/athena_tool/src/athena/athena_ir/sonos_accelerator/buildAccelerator.py:1104: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  print(\"   Conv1 tiles: {:.3f}\".format(100*util_devices_breakdown[0]/devices_breakdown[0])+'%')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cycle 0, started 1 layers, finished 0 layers\n",
      "Cycle 1, started 1 layers, finished 0 layers\n",
      "Cycle 2, started 1 layers, finished 0 layers\n",
      "Cycle 3, started 1 layers, finished 0 layers\n",
      "Cycle 4, started 3 layers, finished 0 layers\n",
      "Cycle 5, started 3 layers, finished 0 layers\n",
      "Cycle 6, started 3 layers, finished 0 layers\n",
      "Cycle 7, started 3 layers, finished 0 layers\n",
      "Cycle 8, started 3 layers, finished 0 layers\n",
      "Cycle 9, started 4 layers, finished 0 layers\n",
      "Cycle 10, started 4 layers, finished 0 layers\n",
      "Cycle 11, started 4 layers, finished 0 layers\n",
      "Cycle 12, started 4 layers, finished 0 layers\n",
      "Cycle 13, started 4 layers, finished 0 layers\n",
      "Cycle 14, started 6 layers, finished 0 layers\n",
      "Cycle 15, started 6 layers, finished 0 layers\n",
      "Cycle 16, started 6 layers, finished 0 layers\n",
      "Cycle 17, started 6 layers, finished 0 layers\n",
      "Cycle 18, started 6 layers, finished 0 layers\n",
      "Cycle 19, started 7 layers, finished 0 layers\n",
      "Cycle 20, started 7 layers, finished 0 layers\n",
      "Cycle 21, started 7 layers, finished 0 layers\n",
      "Cycle 22, started 7 layers, finished 0 layers\n",
      "Cycle 23, started 7 layers, finished 0 layers\n",
      "Cycle 24, started 7 layers, finished 0 layers\n",
      "Cycle 25, started 7 layers, finished 0 layers\n",
      "Cycle 26, started 7 layers, finished 0 layers\n",
      "Cycle 27, started 7 layers, finished 0 layers\n",
      "Cycle 28, started 7 layers, finished 0 layers\n",
      "Cycle 29, started 7 layers, finished 0 layers\n",
      "Cycle 30, started 7 layers, finished 0 layers\n",
      "Cycle 31, started 7 layers, finished 0 layers\n",
      "Cycle 32, started 8 layers, finished 0 layers\n",
      "Cycle 33, started 8 layers, finished 0 layers\n",
      "Cycle 34, started 8 layers, finished 0 layers\n",
      "Cycle 35, started 8 layers, finished 0 layers\n",
      "Cycle 36, started 8 layers, finished 0 layers\n",
      "Cycle 37, started 8 layers, finished 0 layers\n",
      "Cycle 38, started 8 layers, finished 0 layers\n",
      "Cycle 39, started 8 layers, finished 0 layers\n",
      "Cycle 40, started 8 layers, finished 0 layers\n",
      "Cycle 41, started 8 layers, finished 0 layers\n",
      "Cycle 42, started 8 layers, finished 0 layers\n",
      "Cycle 43, started 8 layers, finished 0 layers\n",
      "Cycle 44, started 9 layers, finished 0 layers\n",
      "Cycle 45, started 9 layers, finished 0 layers\n",
      "Cycle 46, started 9 layers, finished 0 layers\n",
      "Cycle 47, started 9 layers, finished 0 layers\n",
      "Cycle 48, started 9 layers, finished 0 layers\n",
      "Cycle 49, started 9 layers, finished 0 layers\n",
      "Cycle 50, started 9 layers, finished 0 layers\n",
      "Cycle 51, started 9 layers, finished 0 layers\n",
      "Cycle 52, started 9 layers, finished 0 layers\n",
      "Cycle 53, started 9 layers, finished 0 layers\n",
      "Cycle 54, started 9 layers, finished 0 layers\n",
      "Cycle 55, started 10 layers, finished 0 layers\n",
      "Cycle 56, started 10 layers, finished 0 layers\n",
      "Cycle 57, started 10 layers, finished 0 layers\n",
      "Cycle 58, started 11 layers, finished 0 layers\n",
      "Cycle 59, started 11 layers, finished 0 layers\n",
      "Cycle 60, started 11 layers, finished 0 layers\n",
      "Cycle 61, started 11 layers, finished 0 layers\n",
      "Cycle 62, started 11 layers, finished 0 layers\n",
      "Cycle 63, started 11 layers, finished 0 layers\n",
      "Cycle 64, started 11 layers, finished 0 layers\n",
      "Cycle 65, started 11 layers, finished 0 layers\n",
      "Cycle 66, started 11 layers, finished 0 layers\n",
      "Cycle 67, started 11 layers, finished 0 layers\n",
      "Cycle 68, started 12 layers, finished 0 layers\n",
      "Cycle 69, started 12 layers, finished 0 layers\n",
      "Cycle 70, started 12 layers, finished 0 layers\n",
      "Cycle 71, started 12 layers, finished 0 layers\n",
      "Cycle 72, started 12 layers, finished 0 layers\n",
      "Cycle 73, started 12 layers, finished 0 layers\n",
      "Cycle 74, started 12 layers, finished 0 layers\n",
      "Cycle 75, started 12 layers, finished 0 layers\n",
      "Cycle 76, started 12 layers, finished 0 layers\n",
      "Cycle 77, started 12 layers, finished 0 layers\n",
      "Cycle 78, started 12 layers, finished 0 layers\n",
      "Cycle 79, started 12 layers, finished 0 layers\n",
      "Cycle 80, started 13 layers, finished 0 layers\n",
      "Cycle 81, started 13 layers, finished 0 layers\n",
      "Cycle 82, started 13 layers, finished 0 layers\n",
      "Cycle 83, started 13 layers, finished 0 layers\n",
      "Cycle 84, started 13 layers, finished 0 layers\n",
      "Cycle 85, started 13 layers, finished 0 layers\n",
      "Cycle 86, started 13 layers, finished 0 layers\n",
      "Cycle 87, started 13 layers, finished 0 layers\n",
      "Cycle 88, started 13 layers, finished 0 layers\n",
      "Cycle 89, started 14 layers, finished 0 layers\n",
      "Cycle 90, started 14 layers, finished 0 layers\n",
      "Cycle 91, started 14 layers, finished 0 layers\n",
      "Cycle 92, started 14 layers, finished 0 layers\n",
      "Cycle 93, started 14 layers, finished 0 layers\n",
      "Cycle 94, started 14 layers, finished 0 layers\n",
      "Cycle 95, started 14 layers, finished 0 layers\n",
      "Cycle 96, started 14 layers, finished 0 layers\n",
      "Cycle 97, started 15 layers, finished 0 layers\n",
      "Cycle 98, started 15 layers, finished 0 layers\n",
      "Cycle 99, started 15 layers, finished 0 layers\n",
      "Cycle 100, started 15 layers, finished 0 layers\n",
      "Cycle 101, started 15 layers, finished 0 layers\n",
      "Cycle 102, started 15 layers, finished 0 layers\n",
      "Cycle 103, started 15 layers, finished 0 layers\n",
      "Cycle 104, started 15 layers, finished 0 layers\n",
      "Cycle 105, started 15 layers, finished 0 layers\n",
      "Cycle 106, started 15 layers, finished 0 layers\n",
      "Cycle 107, started 15 layers, finished 0 layers\n",
      "Cycle 108, started 15 layers, finished 0 layers\n",
      "Cycle 109, started 15 layers, finished 0 layers\n",
      "Cycle 110, started 15 layers, finished 0 layers\n",
      "Cycle 111, started 15 layers, finished 0 layers\n",
      "Cycle 112, started 15 layers, finished 0 layers\n",
      "Cycle 113, started 16 layers, finished 0 layers\n",
      "Cycle 114, started 16 layers, finished 0 layers\n",
      "Cycle 115, started 16 layers, finished 1 layers\n",
      "Cycle 116, started 16 layers, finished 1 layers\n",
      "Cycle 117, started 16 layers, finished 1 layers\n",
      "Cycle 118, started 16 layers, finished 1 layers\n",
      "Cycle 119, started 16 layers, finished 1 layers\n",
      "Cycle 120, started 16 layers, finished 1 layers\n",
      "Cycle 121, started 16 layers, finished 1 layers\n",
      "Cycle 122, started 16 layers, finished 1 layers\n",
      "Cycle 123, started 16 layers, finished 1 layers\n",
      "Cycle 124, started 16 layers, finished 1 layers\n",
      "Cycle 125, started 16 layers, finished 1 layers\n",
      "Cycle 126, started 17 layers, finished 1 layers\n",
      "Cycle 127, started 17 layers, finished 1 layers\n",
      "Cycle 128, started 17 layers, finished 1 layers\n",
      "Cycle 129, started 17 layers, finished 3 layers\n",
      "Cycle 130, started 17 layers, finished 3 layers\n",
      "Cycle 131, started 17 layers, finished 3 layers\n",
      "Cycle 132, started 17 layers, finished 3 layers\n",
      "Cycle 133, started 17 layers, finished 3 layers\n",
      "Cycle 134, started 17 layers, finished 4 layers\n",
      "Cycle 135, started 17 layers, finished 4 layers\n",
      "Cycle 136, started 17 layers, finished 4 layers\n",
      "Cycle 137, started 17 layers, finished 4 layers\n",
      "Cycle 138, started 17 layers, finished 4 layers\n",
      "Cycle 139, started 17 layers, finished 6 layers\n",
      "Cycle 140, started 18 layers, finished 6 layers\n",
      "Cycle 141, started 18 layers, finished 6 layers\n",
      "Cycle 142, started 18 layers, finished 6 layers\n",
      "Cycle 143, started 18 layers, finished 6 layers\n",
      "Cycle 144, started 18 layers, finished 6 layers\n",
      "Cycle 145, started 18 layers, finished 6 layers\n",
      "Cycle 146, started 18 layers, finished 6 layers\n",
      "Cycle 147, started 18 layers, finished 6 layers\n",
      "Cycle 148, started 18 layers, finished 7 layers\n",
      "Cycle 149, started 18 layers, finished 7 layers\n",
      "Cycle 150, started 18 layers, finished 7 layers\n",
      "Cycle 151, started 19 layers, finished 7 layers\n",
      "Cycle 152, started 19 layers, finished 7 layers\n",
      "Cycle 153, started 19 layers, finished 7 layers\n",
      "Cycle 154, started 19 layers, finished 7 layers\n",
      "Cycle 155, started 19 layers, finished 7 layers\n",
      "Cycle 156, started 19 layers, finished 7 layers\n",
      "Cycle 157, started 19 layers, finished 7 layers\n",
      "Cycle 158, started 20 layers, finished 7 layers\n",
      "Cycle 159, started 20 layers, finished 7 layers\n",
      "Cycle 160, started 20 layers, finished 7 layers\n",
      "Cycle 161, started 20 layers, finished 7 layers\n",
      "Cycle 162, started 20 layers, finished 8 layers\n",
      "Cycle 163, started 20 layers, finished 8 layers\n",
      "Cycle 164, started 20 layers, finished 8 layers\n",
      "Cycle 165, started 20 layers, finished 8 layers\n",
      "Cycle 166, started 20 layers, finished 8 layers\n",
      "Cycle 167, started 20 layers, finished 8 layers\n",
      "Cycle 168, started 20 layers, finished 8 layers\n",
      "Cycle 169, started 20 layers, finished 8 layers\n",
      "Cycle 170, started 20 layers, finished 8 layers\n",
      "Cycle 171, started 20 layers, finished 9 layers\n",
      "Cycle 172, started 20 layers, finished 9 layers\n",
      "Cycle 173, started 20 layers, finished 9 layers\n",
      "Cycle 174, started 20 layers, finished 10 layers\n",
      "Cycle 175, started 20 layers, finished 10 layers\n",
      "Cycle 176, started 20 layers, finished 10 layers\n",
      "Cycle 177, started 20 layers, finished 10 layers\n",
      "Cycle 178, started 20 layers, finished 10 layers\n",
      "Cycle 179, started 20 layers, finished 10 layers\n",
      "Cycle 180, started 20 layers, finished 10 layers\n",
      "Cycle 181, started 20 layers, finished 10 layers\n",
      "Cycle 182, started 20 layers, finished 10 layers\n",
      "Cycle 183, started 20 layers, finished 11 layers\n",
      "Cycle 184, started 20 layers, finished 11 layers\n",
      "Cycle 185, started 20 layers, finished 11 layers\n",
      "Cycle 186, started 20 layers, finished 11 layers\n",
      "Cycle 187, started 20 layers, finished 11 layers\n",
      "Cycle 188, started 20 layers, finished 11 layers\n",
      "Cycle 189, started 20 layers, finished 11 layers\n",
      "Cycle 190, started 20 layers, finished 11 layers\n",
      "Cycle 191, started 20 layers, finished 11 layers\n",
      "Cycle 192, started 20 layers, finished 12 layers\n",
      "Cycle 193, started 20 layers, finished 12 layers\n",
      "Cycle 194, started 20 layers, finished 12 layers\n",
      "Cycle 195, started 20 layers, finished 12 layers\n",
      "Cycle 196, started 20 layers, finished 12 layers\n",
      "Cycle 197, started 20 layers, finished 12 layers\n",
      "Cycle 198, started 20 layers, finished 12 layers\n",
      "Cycle 199, started 20 layers, finished 12 layers\n",
      "Cycle 200, started 20 layers, finished 12 layers\n",
      "Cycle 201, started 20 layers, finished 13 layers\n",
      "Cycle 202, started 20 layers, finished 13 layers\n",
      "Cycle 203, started 20 layers, finished 13 layers\n",
      "Cycle 204, started 20 layers, finished 14 layers\n",
      "Cycle 205, started 20 layers, finished 14 layers\n",
      "Cycle 206, started 20 layers, finished 14 layers\n",
      "Cycle 207, started 20 layers, finished 14 layers\n",
      "Cycle 208, started 20 layers, finished 14 layers\n",
      "Cycle 209, started 20 layers, finished 14 layers\n",
      "Cycle 210, started 20 layers, finished 14 layers\n",
      "Cycle 211, started 20 layers, finished 14 layers\n",
      "Cycle 212, started 20 layers, finished 14 layers\n",
      "Cycle 213, started 20 layers, finished 14 layers\n",
      "Cycle 214, started 20 layers, finished 14 layers\n",
      "Cycle 215, started 20 layers, finished 14 layers\n",
      "Cycle 216, started 20 layers, finished 14 layers\n",
      "Cycle 217, started 20 layers, finished 14 layers\n",
      "Cycle 218, started 20 layers, finished 14 layers\n",
      "Cycle 219, started 20 layers, finished 14 layers\n",
      "Cycle 220, started 20 layers, finished 14 layers\n",
      "Cycle 221, started 20 layers, finished 15 layers\n",
      "Cycle 222, started 20 layers, finished 15 layers\n",
      "Cycle 223, started 20 layers, finished 15 layers\n",
      "Cycle 224, started 20 layers, finished 15 layers\n",
      "Cycle 225, started 20 layers, finished 15 layers\n",
      "Cycle 226, started 20 layers, finished 15 layers\n",
      "Cycle 227, started 20 layers, finished 15 layers\n",
      "Cycle 228, started 20 layers, finished 15 layers\n",
      "Cycle 229, started 20 layers, finished 15 layers\n",
      "Cycle 230, started 20 layers, finished 15 layers\n",
      "Cycle 231, started 20 layers, finished 15 layers\n",
      "Cycle 232, started 20 layers, finished 15 layers\n",
      "Cycle 233, started 20 layers, finished 15 layers\n",
      "Cycle 234, started 20 layers, finished 16 layers\n",
      "Cycle 235, started 20 layers, finished 16 layers\n",
      "Cycle 236, started 20 layers, finished 16 layers\n",
      "Cycle 237, started 20 layers, finished 16 layers\n",
      "Cycle 238, started 20 layers, finished 16 layers\n",
      "Cycle 239, started 20 layers, finished 16 layers\n",
      "Cycle 240, started 20 layers, finished 16 layers\n",
      "Cycle 241, started 20 layers, finished 16 layers\n",
      "Cycle 242, started 20 layers, finished 16 layers\n",
      "Cycle 243, started 20 layers, finished 16 layers\n",
      "Cycle 244, started 20 layers, finished 16 layers\n",
      "Cycle 245, started 20 layers, finished 16 layers\n",
      "Cycle 246, started 20 layers, finished 17 layers\n",
      "Cycle 247, started 20 layers, finished 17 layers\n",
      "Cycle 248, started 20 layers, finished 17 layers\n",
      "Cycle 249, started 20 layers, finished 18 layers\n",
      "Cycle 250, started 20 layers, finished 18 layers\n",
      "Cycle 251, started 20 layers, finished 18 layers\n",
      "Cycle 252, started 20 layers, finished 18 layers\n",
      "Cycle 253, started 20 layers, finished 18 layers\n",
      "Cycle 254, started 20 layers, finished 19 layers\n",
      "Cycle 255, started 20 layers, finished 19 layers\n",
      "Cycle 256, started 21 layers, finished 20 layers\n",
      "Cycle 257, started 21 layers, finished 20 layers\n",
      "Cycle 258, started 21 layers, finished 20 layers\n",
      "Cycle 259, started 21 layers, finished 20 layers\n",
      "Cycle 260, started 21 layers, finished 20 layers\n",
      "Cycle 261, started 22 layers, finished 21 layers\n",
      "Cycle 262, started 22 layers, finished 21 layers\n",
      "Cycle 263, started 23 layers, finished 22 layers\n",
      "Cycle 264, started 23 layers, finished 22 layers\n",
      "Cycle 265, started 23 layers, finished 22 layers\n",
      "Cycle 266, started 23 layers, finished 22 layers\n",
      "Cycle 267, started 23 layers, finished 22 layers\n",
      "Cycle 268, started 24 layers, finished 23 layers\n",
      "Cycle 269, started 24 layers, finished 23 layers\n",
      "Cycle 270, started 24 layers, finished 24 layers\n",
      "////\n",
      "Latency: 271 cycles\n",
      "Delay between examples: 131 cycles\n",
      "Compute density: 0.504 TOPS/mm^2\n",
      "Compute density (streaming): 1.042 TOPS/mm^2\n",
      "Simulation time: 157.506s\n",
      "----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mplagge/miniconda3/envs/athena_dev/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/mplagge/miniconda3/envs/athena_dev/lib/python3.9/site-packages/numpy/core/_methods.py:180: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = um.true_divide(\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './dataflow_outputs/VGG16_1152x256_112cycles.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2685/341360244.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcli\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_2685/487029107.py\u001b[0m in \u001b[0;36mcli\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m    245\u001b[0m                 \u001b[0;31m# Simulate data flow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m                 \u001b[0mactivity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffersizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraffic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menergies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspeed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimulateDataflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNN_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlayerParams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtiles\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0marchParams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m                 \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavez\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./dataflow_outputs/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0marchName\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mactivity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbuffersizes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffersizes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtraffic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraffic\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menergies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menergies\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mspeed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;31m# Plot dataflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msavez\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/athena_dev/lib/python3.9/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msavez\u001b[0;34m(file, *args, **kwds)\u001b[0m\n\u001b[1;32m    615\u001b[0m     \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m     \"\"\"\n\u001b[0;32m--> 617\u001b[0;31m     \u001b[0m_savez\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/athena_dev/lib/python3.9/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36m_savez\u001b[0;34m(file, args, kwds, compress, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    711\u001b[0m         \u001b[0mcompression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZIP_STORED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m     \u001b[0mzipf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzipfile_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"w\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnamedict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/athena_dev/lib/python3.9/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mzipfile_factory\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allowZip64'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/athena_dev/lib/python3.9/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[1;32m   1237\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1240\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './dataflow_outputs/VGG16_1152x256_112cycles.npz'"
     ]
    }
   ],
   "source": [
    "cli()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02e3ef6-2a7a-48a3-9b50-9c023c27b70b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
